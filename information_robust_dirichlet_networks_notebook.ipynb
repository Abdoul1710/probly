{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad65fcd7",
   "metadata": {},
   "source": [
    "# Information Robust Dirichlet Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb53a5",
   "metadata": {},
   "source": [
    "In this notebook, we implement the specialized training loss proposed in the paper _Information Robust Dirichlet Networks for Predictive Uncertainty Estimation_ by Tsiligkaridis (2019). The method models predictive uncertainty by having a neural network output Dirichlet concentration parameters ð›¼ instead of just a pointwise softmax.\n",
    "\n",
    "The total loss is composed of three terms:\n",
    "\n",
    "1. Calibration term: implemented in the function  lp_fn\n",
    "2. Regularization term: implemented in the function  regularization_fn\n",
    "3. Adversiarial Entropy penalty: implemented in the function  dirichlet_entropy\n",
    "\n",
    "In the paper and in this notenbook, L_p loss is not directly computed but rather an upper bound for it, denoted by F_i (for sample i)  \n",
    "\n",
    "The regularization term penalizes high alpha values for incorrect classes.  \n",
    "\n",
    "The final term uses the alpha values the model assigns to adversarial inputs.\n",
    "The model is rewarded for outputting a Dirichlet-distribution with high entropy on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c812c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.special import digamma\n",
    "\n",
    "def lp_fn(alpha, y, p):\n",
    "    \"\"\"\n",
    "    Computes F_i (one per sample) using the expectation-based formulation:\n",
    "        F_i = ( E[(1-p_c)^p] + Î£_{jâ‰ c} E[p_j^p] )^(1/p)\n",
    "        Then sums all F_i \n",
    "    Args:\n",
    "        alpha : (B, K) Dirichlet concentration parameters (>0)\n",
    "        y     : (B, K) one-hot labels\n",
    "        p     : scalar exponent\n",
    "\n",
    "    Returns:\n",
    "        Î£_i F_i : Float\n",
    "    \"\"\"\n",
    "\n",
    "    B, K = alpha.shape\n",
    "\n",
    "    # total concentration Î±0\n",
    "    alpha0 = alpha.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # extract Î±_c (correct class)\n",
    "    alpha_c = (alpha * y).sum(dim=1, keepdim=True)  # (B,1)\n",
    "    alpha0_minus_c = alpha0 - alpha_c               # (B,1)\n",
    "\n",
    "    # log B(a,b) used for expectations: E[X^p] = B(a+p,b)/B(a,b)\n",
    "    def logB(a, b):\n",
    "        return torch.lgamma(a) + torch.lgamma(b) - torch.lgamma(a + b)\n",
    "\n",
    "    # E[(1 - p_c)^p]   where (1 - p_c) ~ Beta( Î±0 - Î±_c , Î±_c )\n",
    "    log_E1 = logB(alpha0_minus_c + p, alpha_c) - logB(alpha0_minus_c, alpha_c)\n",
    "    E1 = torch.exp(log_E1)  # (B,1)\n",
    "\n",
    "    # Per-class E[p_j^p] for all j\n",
    "    log_Ep = logB(alpha + p, alpha0 - alpha) - logB(alpha, alpha0 - alpha)  # (B,K)\n",
    "    Ep = torch.exp(log_Ep)\n",
    "\n",
    "    # zero-out the true class term so we sum only jâ‰ c\n",
    "    Ep = Ep * (1 - y)\n",
    "\n",
    "    # final expectation sum\n",
    "    E_sum = E1 + Ep.sum(dim=1, keepdim=True)  # (B,1)\n",
    "\n",
    "    # apply ^(1/p)\n",
    "    Fi = E_sum.pow(1.0 / p).squeeze(1)  # (B,)\n",
    "\n",
    "    return Fi.sum()\n",
    "\n",
    "\n",
    "def regularization_fn(alpha, y):\n",
    "    \"\"\"\n",
    "    Implements the regularization term using trigamma functions and an equivalent formulation of the regularization term.\n",
    "\n",
    "    alpha: (B, K)   Dirichlet parameters > 0\n",
    "    y:     (B, K)   one-hot labels\n",
    "    \n",
    "        returns:\n",
    "            Float\n",
    "    \"\"\"\n",
    "\n",
    "    # Build alpha_tilde by replacing correct-class alpha with 1\n",
    "    alpha_tilde = alpha * (1 - y) + y\n",
    "\n",
    "    # Compute alpha_tilde_0 = 1 + sum over incorrect classes\n",
    "    alpha_tilde_0 = torch.sum(alpha_tilde, dim=1, keepdim=True)\n",
    "\n",
    "    # Polygamma(1, x) = trigamma(x)\n",
    "    trigamma_alpha = torch.polygamma(1, alpha_tilde)\n",
    "    trigamma_alpha0 = torch.polygamma(1, alpha_tilde_0)\n",
    "\n",
    "    # (alpha_tilde - 1)^2 term\n",
    "    diff_sq = (alpha_tilde - 1.0)**2\n",
    "\n",
    "    # Penalty only for incorrect classes â†’ mask out true class\n",
    "    mask = (1 - y)\n",
    "\n",
    "    # Compute elementwise contribution\n",
    "    term = 0.5 * diff_sq * (trigamma_alpha - trigamma_alpha0) * mask\n",
    "\n",
    "    # Sum over classes and batch\n",
    "    return torch.sum(torch.sum(term, dim=1))\n",
    "\n",
    "\n",
    "def dirichlet_entropy(alpha):\n",
    "    \"\"\"\n",
    "    alpha: tensor of shape (B_a, K), where B_a = number of adversarial inputs\n",
    "    returns: entropy : Float\n",
    "    \"\"\"\n",
    "    K = alpha.size(-1)\n",
    "    alpha0 = alpha.sum(dim=-1)\n",
    "\n",
    "    log_B = torch.lgamma(alpha).sum(dim=-1) - torch.lgamma(alpha0)\n",
    "\n",
    "    term1 = log_B\n",
    "    term2 = (alpha0 - K) * digamma(alpha0)\n",
    "    term3 = ((alpha - 1) * digamma(alpha)).sum(dim=-1)\n",
    "    entropy = term1 + term2 - term3\n",
    "\n",
    "    return entropy.sum()\n",
    "\n",
    "def loss_IRD(alpha, y, adversarial_alpha = None, p = 2, lam = 1.0, gamma = 1.0):\n",
    "    \"\"\"\n",
    "    Computes the Loss introduced in paper: Information Robust Dirichlet Networks for Predictive Uncertainty Estimation\n",
    "    Args:\n",
    "        alpha : (B, K) Dirichlet concentration parameters\n",
    "        adversarial_alpha : (B_a, K) adversarial_alpha concentration parameters for adversarial inputs\n",
    "        y     : (B, K) one-hot labels \n",
    "        p     : scalar exponent\n",
    "    Returns:\n",
    "        loss_IRD : the IRD loss comprised of all three terms, summed over all input examples\n",
    "    \"\"\"\n",
    "    lp_term = lp_fn(alpha, y, p)\n",
    "    reg_term = regularization_fn(alpha, y)\n",
    "    if adversarial_alpha is not None:\n",
    "        entropy_term = dirichlet_entropy(adversarial_alpha)\n",
    "    else:\n",
    "        entropy_term = 0.0\n",
    "        \n",
    "    return lp_term + lam * reg_term - gamma * entropy_term\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d00218f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6637, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# small test\n",
    "alpha = torch.tensor([[2.5, 1.2, 0.8],\n",
    "                      [1.2, 4.0, 1.3]], requires_grad=True)\n",
    "\n",
    "adversarial_alpha = torch.tensor([[5, 4, 3],\n",
    "                      [1.2, 2.0, 1.3]], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([[1,0,0],\n",
    "                  [0,1,0]], dtype=torch.float32)\n",
    "\n",
    "loss = loss_IRD(alpha, y, adversarial_alpha)\n",
    "print(loss)            # tensor of shape (B,)\n",
    "loss.mean().backward() # gradients flow properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa1340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
